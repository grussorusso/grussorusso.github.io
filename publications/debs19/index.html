<!doctype html><html lang><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Reinforcement learning based policies for elastic stream processing on heterogeneous resources | Gabriele Russo Russo</title><meta name=description content><meta name=author content="Gabriele Russo Russo"><meta name=google-site-verification content="bPwo8jbhRQyTnf8PZdjYiBqNZ109q7xM6askN5vfJic"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css integrity=sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2 crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin=anonymous><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css><link rel=stylesheet href=https://grussorusso.github.io/sass/researcher.min.css></head><body><nav class="navbar navbar-expand-md navbar-light" style=background-color:#fff;padding:2px><div class="container mt-3"><a class=custom-brand href=https://grussorusso.github.io/>Gabriele Russo Russo</a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navbarTogglerDemo02 aria-controls=navbarTogglerDemo02 aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarTogglerDemo02><ul class="navbar-nav ml-auto mt-2 mt-md-0"><li class=nav-item><a class=custom-nav-link href=https://grussorusso.github.io/ title=Home>Home</a></li><li class=nav-item><a class=custom-nav-link href=https://grussorusso.github.io/cv.pdf title=CV>CV</a></li><li class=nav-item><a class=custom-nav-link href=https://grussorusso.github.io/publications title=Publications>Publications</a></li><li class=nav-item><a class=custom-nav-link href=https://grussorusso.github.io/software title=Software>Software</a></li><li class=nav-item><a class=custom-nav-link href=https://grussorusso.github.io/teaching title=Teaching>Teaching</a></li></ul></div></div></nav><hr style=margin-bottom:1.1em;margin-top:1.1em><div id=content><div class=container><a href=https://grussorusso.github.io/publications#reinforcement-learning-based-policies-for-elastic-stream-processing-on-heterogeneous-resources>back</a><h2>Reinforcement learning based policies for elastic stream processing on heterogeneous resources</h2><p>G. Russo Russo, V. Cardellini, F. Lo Presti</p><p class=paper-info>Proc. of DEBS 2019</p><p><a href=https://doi.org/10.1145/3328905.3329506>[doi]</i></i></a></p><p>Data Stream Processing (DSP) has emerged as a key enabler to develop pervasive services that require to process data in a near real-time fashion. DSP applications keep up with the high volume of produced data by scaling their execution on multiple computing nodes, so as to process the incoming data flow in parallel. Workloads variability requires to elastically adapt the application parallelism at run-time in order to avoid over-provisioning. Elasticity policies for DSP have been widely investigated, but mostly under the simplifying assumption of homogeneous infrastructures. The resulting solutions do not capture the richness and inherent complexity of modern infrastructures, where heterogeneous computing resources are available on-demand. In this paper, we formulate the problem of controlling elasticity on heterogeneous resources as a Markov Decision Process (MDP). The resulting MDP is not easily solved by traditional techniques due to state space explosion, and thus we show how linear Function Approximation and Tile Coding can be used to efficiently compute elasticity policies at run-time. In order to deal with parameters uncertainty, we integrate the proposed approach with Reinforcement Learning algorithms. Our numerical evaluation shows the efficacy of the presented solutions compared to standard methods in terms of accuracy and convergence speed.</p></div></div><div id=footer class=mb-5><hr><div class="container text-center"><a href=https://github.com/grussorusso class="fab fa-github fa-1x" title=GitHub></a>
<a href="https://scholar.google.com/citations?user=rRiNfS0AAAAJ" class="ai ai-google-scholar fa-1x" title=Scholar></a>
<a href=https://dblp.org/pid/214/1442.html class="ai ai-dblp fa-1x" title=DBLP></a></div><div class="container text-center"><a href=https://grussorusso.github.io/ title><small></small></a></div></div><script src=https://code.jquery.com/jquery-3.5.1.slim.min.js integrity=sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js integrity=sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx crossorigin=anonymous></script></body></html>